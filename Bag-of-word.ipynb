{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Johnny\n",
      "[nltk_data]     Peng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623\n",
      "406\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "data_full = pd.read_csv(\"data.csv\")\n",
    "X_train_df = data_full.sample(frac=0.8,random_state=200)\n",
    "\n",
    "X_train = np.array(X_train_df['text'])\n",
    "Y_train = np.array(X_train_df['label'])\n",
    "print(len(X_train))\n",
    "\n",
    "X_test = np.array(data_full.loc[~data_full.index.isin(X_train_df.index)]['text'])\n",
    "Y_test = np.array(data_full.loc[~data_full.index.isin(X_train_df.index)]['label'])\n",
    "print(len(Y_test))\n",
    "\n",
    "t_size = int(len(X_train)*90/100)\n",
    "print(len(X_train[:t_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_clean shape: (1623,)\n",
      "X_test_clean shape: (406,)\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = []\n",
    "X_test_clean = []\n",
    "\n",
    "bad_chars = [';', ':', '!', \"*\",\"'\", '[',']', '(', ')',',','?','’','-','|','\"','$','•','\\xa0'] \n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    temp = X_train[i].lower()\n",
    "    for j in bad_chars:\n",
    "        temp = temp.replace(j,'')\n",
    "    X_train_clean.append(temp)\n",
    "\n",
    "for i in range(0,len(X_test)):\n",
    "    temp = X_test[i].lower()\n",
    "    for j in bad_chars:\n",
    "        temp = temp.replace(j,'')\n",
    "    X_test_clean.append(temp)\n",
    "\n",
    "# Text data\n",
    "X_train_clean = np.array(X_train_clean)\n",
    "X_test_clean = np.array(X_test_clean)\n",
    "\n",
    "print (\"X_train_clean shape: \" + str(X_train_clean.shape))\n",
    "print (\"X_test_clean shape: \" + str(X_test_clean.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2029,)\n",
      "stormfloodrainwaterhailorwind water damage due to excessive downpour the gutters couldnt cope with excessive rain and appears they overflowed under the eave. appears to just be wet ceiling in one room with only a few drops coming through into the room so at this stage just looks like minor plaster patching and painting required\n"
     ]
    }
   ],
   "source": [
    "X_full = np.hstack((X_train_clean,X_test_clean))\n",
    "print(X_full.shape)\n",
    "print(X_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3998\n",
      "[('stormfloodrainwaterhailorwind', 692), ('accidentallossordamage', 530), ('damage', 401), ('water', 331), ('insured', 317), ('electricmotorburnout', 195), ('damaged', 189), ('storm', 186), ('theftattemptedtheftorburglary', 163), ('escapeofliquidexcludingflood', 149), ('door', 146), ('house', 139), ('caused', 135), ('air', 126), ('home', 125), ('roof', 122), ('attached', 121), ('pump', 121), ('client', 121), ('ceiling', 119), ('back', 118), ('quote', 110), ('lost', 107), ('power', 100), ('due', 98), ('pool', 97), ('accidentalbreakage', 95), ('ring', 94), ('causing', 94), ('garage', 93), ('motor', 93), ('car', 88), ('tv', 88), ('claim', 87), ('phone', 86), ('also', 85), ('items', 85), ('working', 81), ('impact', 79), ('stolen', 78), ('fusion', 77), ('laptop', 77), ('carpet', 75), ('report', 73), ('accidentally', 72), ('room', 69), ('found', 69), ('electrical', 68), ('lightningorthunderbolt', 68), ('dropped', 67)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(text, vocab):\n",
    "    tokens = clean_doc(text)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(docs, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for i in range(0,len(docs)):\n",
    "        add_doc_to_vocab(docs[i], vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs(list(X_full), vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2062\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w',encoding=\"utf-8\")\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    " \n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1623 406\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r',encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(text, vocab):\n",
    "    # clean doc\n",
    "    tokens = clean_doc(text)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(docs, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for i in range(0,len(docs)):\n",
    "        line = doc_to_line(docs[i], vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load all training reviews\n",
    "X_train_lines = process_docs(list(X_train_clean), vocab)\n",
    "X_test_lines = process_docs(list(X_test_clean), vocab)\n",
    "# summarize what we have\n",
    "print(len(X_train_lines), len(X_test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1623, 1998)\n",
      "(406, 1998)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(X_train_lines)\n",
    "\n",
    "# encode training data set\n",
    "X_train_token = tokenizer.texts_to_matrix(X_train_lines, mode='count')\n",
    "print(X_train_token.shape)\n",
    "\n",
    "# encode training data set\n",
    "X_test_token = tokenizer.texts_to_matrix(X_test_lines, mode='count')\n",
    "print(X_test_token.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(np.insert(np.array(list(tokenizer.word_index)),0,'0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7597042513863216"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion= 'gini', max_depth= 5, splitter= 'best',min_samples_leaf=10)\n",
    "clf = clf.fit(X_train_token, Y_train)\n",
    "predicts = clf.predict(X_train_token)\n",
    "count = 0 \n",
    "\n",
    "for i in range(0,Y_train.shape[0]):\n",
    "    if predicts[i] == Y_train[i]:\n",
    "        count = count + 1\n",
    "\n",
    "count/Y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7733990147783252"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "predicts_test = clf.predict(X_test_token)\n",
    "\n",
    "for i in range(0,Y_test.shape[0]):\n",
    "    if predicts_test[i] == Y_test[i]:\n",
    "        count = count + 1\n",
    "\n",
    "count/Y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ClaimDesc.pdf'"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=names,  \n",
    "                      class_names=list(('Building','Contents','Both')),  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"ClaimDesc\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny Peng\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search mean and stdev:\n",
      "\n",
      "0.734 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 10, 'splitter': 'best'}\n",
      "0.731 (+/-0.024) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 10, 'splitter': 'random'}\n",
      "0.734 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 15, 'splitter': 'best'}\n",
      "0.734 (+/-0.023) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 15, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 20, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 20, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 25, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 25, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 30, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 30, 'splitter': 'random'}\n",
      "0.738 (+/-0.033) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10, 'splitter': 'best'}\n",
      "0.737 (+/-0.034) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 10, 'splitter': 'random'}\n",
      "0.735 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 15, 'splitter': 'best'}\n",
      "0.735 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 15, 'splitter': 'random'}\n",
      "0.733 (+/-0.027) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 20, 'splitter': 'best'}\n",
      "0.733 (+/-0.027) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 20, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 25, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 25, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 30, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 4, 'min_samples_leaf': 30, 'splitter': 'random'}\n",
      "0.743 (+/-0.033) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'splitter': 'best'}\n",
      "0.742 (+/-0.034) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 10, 'splitter': 'random'}\n",
      "0.736 (+/-0.022) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 15, 'splitter': 'best'}\n",
      "0.735 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 15, 'splitter': 'random'}\n",
      "0.737 (+/-0.024) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'splitter': 'best'}\n",
      "0.736 (+/-0.044) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 20, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 25, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 25, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 30, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 30, 'splitter': 'random'}\n",
      "0.737 (+/-0.030) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 10, 'splitter': 'best'}\n",
      "0.738 (+/-0.026) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 10, 'splitter': 'random'}\n",
      "0.737 (+/-0.030) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 15, 'splitter': 'best'}\n",
      "0.739 (+/-0.018) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 15, 'splitter': 'random'}\n",
      "0.739 (+/-0.029) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 20, 'splitter': 'best'}\n",
      "0.739 (+/-0.029) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 20, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 25, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 25, 'splitter': 'random'}\n",
      "0.732 (+/-0.025) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 30, 'splitter': 'best'}\n",
      "0.732 (+/-0.025) for {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 30, 'splitter': 'random'}\n",
      "0.743 (+/-0.027) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 10, 'splitter': 'best'}\n",
      "0.744 (+/-0.027) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 10, 'splitter': 'random'}\n",
      "0.743 (+/-0.027) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 15, 'splitter': 'best'}\n",
      "0.745 (+/-0.041) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 15, 'splitter': 'random'}\n",
      "0.739 (+/-0.030) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 20, 'splitter': 'best'}\n",
      "0.733 (+/-0.027) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 20, 'splitter': 'random'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 25, 'splitter': 'best'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 25, 'splitter': 'random'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 30, 'splitter': 'best'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 30, 'splitter': 'random'}\n",
      "0.748 (+/-0.039) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'splitter': 'best'}\n",
      "0.751 (+/-0.025) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'splitter': 'random'}\n",
      "0.748 (+/-0.039) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 15, 'splitter': 'best'}\n",
      "0.739 (+/-0.029) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 15, 'splitter': 'random'}\n",
      "0.746 (+/-0.040) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'splitter': 'best'}\n",
      "0.742 (+/-0.040) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 20, 'splitter': 'random'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 25, 'splitter': 'best'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 25, 'splitter': 'random'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 30, 'splitter': 'best'}\n",
      "0.734 (+/-0.031) for {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 30, 'splitter': 'random'}\n",
      "\n",
      "Best parameters: {'criterion': 'gini', 'max_depth': 5, 'min_samples_leaf': 10, 'splitter': 'random'}\n",
      "\n",
      "Classification report (clear):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82       224\n",
      "           1       0.76      0.76      0.76       145\n",
      "           2       0.42      0.14      0.20        37\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       406\n",
      "   macro avg       0.65      0.59      0.60       406\n",
      "weighted avg       0.74      0.76      0.74       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# grid search\n",
    "param_grid = [\n",
    " {'max_depth': [3,4,5],\n",
    " 'criterion': ['entropy', 'gini'],\n",
    " 'splitter': ['best', 'random'],\n",
    " 'min_samples_leaf': [10,15,20,25,30]}\n",
    "]\n",
    "tree = GridSearchCV(DecisionTreeClassifier(), param_grid)\n",
    "tree.fit(X_train_token, np.array(X_train_df['label']))\n",
    "# Print grid search results\n",
    "print('Grid search mean and stdev:\\n')\n",
    "for i in range(0,len(tree.cv_results_[\"params\"])):\n",
    " print(\"{:0.3f} (+/-{:0.03f}) for {}\".format(\n",
    " tree.cv_results_[\"mean_test_score\"][i], tree.cv_results_[\"std_test_score\"][i] *\n",
    "2, tree.cv_results_[\"params\"][i]))\n",
    "# best params\n",
    "print('\\nBest parameters:', tree.best_params_)\n",
    "# Evaluate on test set\n",
    "print('\\nClassification report ({}):\\n'.format(key))\n",
    "print(classification_report(Y_test, tree.predict(X_test_token)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
